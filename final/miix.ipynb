{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약 및 키워드 추출 모듈화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 프로젝트 규모 커지면\n",
    "\n",
    "- TextSummarizer/\n",
    "- ├── __init__.py\n",
    "- ├── exceptions.py    # 커스텀 예외 클래스 분리\n",
    "- └── summarizer.py    # 주요 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from typing import Union, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "class TextSummarizer:\n",
    "    import torch  # PyTorch 임포트 추가\n",
    "\n",
    "class TextSummarizer:\n",
    "    def __init__(self, model_name: str = 'digit82/kobart-summarization', device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        텍스트 요약기 초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): 허깅페이스 모델 이름/경로 \n",
    "            device (str): 'auto'(기본값), 'cuda', 'cpu' 중 선택\n",
    "                         'auto'시 CUDA 가용성 자동 감지\n",
    "        \"\"\"\n",
    "        # 디바이스 자동 설정\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device.lower()  # 입력 대소문자 통일\n",
    "            \n",
    "        # CUDA 요청했지만 사용 불가능한 경우 경고 출력\n",
    "        if self.device.startswith('cuda') and not torch.cuda.is_available():\n",
    "            print(\"⚠️ 경고: CUDA를 사용할 수 없어 CPU로 대체됩니다.\")\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "            self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model = self.model.to(self.device)  # 선택된 디바이스로 모델 이동\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ModelLoadError(f\"모델 로딩 실패: {str(e)}\") from e\n",
    "\n",
    "    # 이하 기존 메서드들 동일하게 유지...\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        텍스트 전처리 수행\n",
    "        \n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            str: 전처리된 텍스트\n",
    "            \n",
    "        Raises:\n",
    "            EmptyTextError: 입력 텍스트가 비어 있을 경우\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            raise EmptyTextError(\"전처리 입력 텍스트가 비어 있습니다.\")\n",
    "            \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'\\d+\\.\\d+', '', text)\n",
    "        text = re.sub(r'[●◆▶▼]+', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def chunk_text(self, text: str, sentence_per_chunk: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        텍스트를 청크 단위로 분할\n",
    "        \n",
    "        Args:\n",
    "            text (str): 입력 텍스트\n",
    "            sentence_per_chunk (int): 청크당 문장 수 (기본값: 10)\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 분할된 텍스트 청크 리스트\n",
    "            \n",
    "        Raises:\n",
    "            InvalidChunkSizeError: 유효하지 않은 청크 크기 지정시\n",
    "        \"\"\"\n",
    "        if sentence_per_chunk < 1:\n",
    "            raise InvalidChunkSizeError(\"청크 크기는 1 이상이어야 합니다.\")\n",
    "            \n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        return [' '.join(sentences[i:i+sentence_per_chunk]).strip() \n",
    "                for i in range(0, len(sentences), sentence_per_chunk)]\n",
    "\n",
    "    def summarize_chunk(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        단일 텍스트 청크 요약\n",
    "        \n",
    "        Args:\n",
    "            text (str): 입력 텍스트 청크\n",
    "            \n",
    "        Returns:\n",
    "            str: 요약 결과\n",
    "            \n",
    "        Raises:\n",
    "            GenerationError: 텍스트 생성 실패시\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_ids = self.tokenizer.encode(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "            ).to(self.device)\n",
    "            \n",
    "            summary_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                num_beams=6,\n",
    "                max_length=150,\n",
    "                min_length=80,\n",
    "                repetition_penalty=3.0,\n",
    "                length_penalty=0.8,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=4\n",
    "            )\n",
    "            return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            raise GenerationError(f\"텍스트 생성 실패: {str(e)}\") from e\n",
    "\n",
    "    def hierarchical_summarize(self, text: str, depth: int = 2) -> str:\n",
    "        \"\"\"\n",
    "        계층적 요약 수행\n",
    "        \n",
    "        Args:\n",
    "            text (str): 입력 텍스트\n",
    "            depth (int): 요약 깊이 (기본값: 2)\n",
    "            \n",
    "        Returns:\n",
    "            str: 계층적 요약 결과\n",
    "            \n",
    "        Raises:\n",
    "            InvalidDepthError: 유효하지 않은 depth 값 입력시\n",
    "            RecursionDepthExceeded: 과도한 재귀 깊이 지정시\n",
    "        \"\"\"\n",
    "        if depth < 0:\n",
    "            raise InvalidDepthError(\"depth는 0 이상의 정수여야 합니다.\")\n",
    "        if depth > 5:\n",
    "            raise RecursionDepthExceeded(\"최대 허용 depth(5)를 초과했습니다.\")\n",
    "            \n",
    "        if depth == 0:\n",
    "            return self.summarize_chunk(text)\n",
    "            \n",
    "        chunks = self.chunk_text(text)\n",
    "        summaries = [self.hierarchical_summarize(chunk, depth-1) for chunk in chunks]\n",
    "        return self.summarize_chunk(' '.join(summaries))\n",
    "\n",
    "    def postprocess(self, summary: str) -> str:\n",
    "        \"\"\"\n",
    "        요약 결과 후처리\n",
    "        \n",
    "        Args:\n",
    "            summary (str): 원본 요약 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            str: 후처리된 최종 요약문\n",
    "        \"\"\"\n",
    "        if not summary:\n",
    "            return \"\"\n",
    "            \n",
    "        sentences = list(dict.fromkeys(summary.split('. ')))\n",
    "        return '. '.join(sentences).replace(' ,', ',')\n",
    "\n",
    "    def extract_pdf(self, file_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        PDF 파일에서 텍스트 추출\n",
    "        \n",
    "        Args:\n",
    "            file_path (Union[str, Path]): PDF 파일 경로\n",
    "            \n",
    "        Returns:\n",
    "            str: 추출된 텍스트\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: 파일이 존재하지 않을 경우\n",
    "            InvalidPDFFileError: 유효하지 않은 PDF 파일일 경우\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "            \n",
    "        try:\n",
    "            doc = fitz.open(file_path)\n",
    "            extracted = [page.get_text() for page in doc]\n",
    "            return str(extracted).replace('·','\\t')\n",
    "        except Exception as e:\n",
    "            raise InvalidPDFFileError(f\"PDF 파싱 실패: {str(e)}\") from e\n",
    "\n",
    "    def summarize(self, input_data: Union[str, Path], depth: int = 2) -> str:\n",
    "        \"\"\"\n",
    "        요약 메인 메서드\n",
    "        \n",
    "        Args:\n",
    "            input_data (Union[str, Path]): 입력 텍스트 또는 PDF 파일 경로\n",
    "            depth (int): 요약 깊이 (기본값: 2)\n",
    "            \n",
    "        Returns:\n",
    "            str: 최종 요약 텍스트\n",
    "            \n",
    "        Raises:\n",
    "            InvalidInputError: 유효하지 않은 입력 타입시\n",
    "        \"\"\"\n",
    "        if not isinstance(input_data, (str, Path)):\n",
    "            raise InvalidInputError(\"입력은 문자열 또는 파일 경로여야 합니다.\")\n",
    "            \n",
    "        try:\n",
    "            # 파일 경로인 경우 PDF 추출\n",
    "            if isinstance(input_data, (str, Path)) and os.path.isfile(input_data):\n",
    "                if not str(input_data).lower().endswith('.pdf'):\n",
    "                    raise InvalidFileTypeError(\"PDF 파일만 지원합니다.\")\n",
    "                raw_text = self.extract_pdf(input_data)\n",
    "            else:\n",
    "                raw_text = input_data\n",
    "                \n",
    "            cleaned = self.preprocess(raw_text)\n",
    "            summarized = self.hierarchical_summarize(cleaned, depth=depth)\n",
    "            return self.postprocess(summarized)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise SummarizationError(f\"요약 처리 실패: {str(e)}\") from e\n",
    "\n",
    "# Custom Exceptions ----------------------------------------------------------\n",
    "class TextSummarizerError(Exception):\n",
    "    \"\"\"요약기 기본 예외 클래스\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelLoadError(TextSummarizerError):\n",
    "    \"\"\"모델 로딩 실패 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class EmptyTextError(TextSummarizerError):\n",
    "    \"\"\"빈 텍스트 입력 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidChunkSizeError(TextSummarizerError):\n",
    "    \"\"\"잘못된 청크 크기 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class GenerationError(TextSummarizerError):\n",
    "    \"\"\"텍스트 생성 실패 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidDepthError(TextSummarizerError):\n",
    "    \"\"\"잘못된 depth 값 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class RecursionDepthExceeded(TextSummarizerError):\n",
    "    \"\"\"과도한 재귀 깊이 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidPDFFileError(TextSummarizerError):\n",
    "    \"\"\"잘못된 PDF 파일 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidInputError(TextSummarizerError):\n",
    "    \"\"\"잘못된 입력 타입 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidFileTypeError(TextSummarizerError):\n",
    "    \"\"\"지원하지 않는 파일 타입 예외\"\"\"\n",
    "    pass\n",
    "\n",
    "class SummarizationError(TextSummarizerError):\n",
    "    \"\"\"일반 요약 오류 예외\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용 예시\n",
    "summarizer = TextSummarizer()\n",
    "\n",
    "# 1. PDF 파일 요약\n",
    "summary_from_pdf = summarizer.summarize('./example_1.pdf')\n",
    "print(\"PDF 요약 결과:\\n\", summary_from_pdf)\n",
    "\n",
    "# 2. 일반 텍스트 요약\n",
    "text = \"\"\"\n",
    "정부는 오늘 새로운 인공지능 산업 전략을 발표했다. 이 전략은 국내 기업의 기술 자립을 촉진하고, \n",
    "글로벌 경쟁력을 확보하기 위한 다양한 정책을 포함하고 있다. 특히 데이터 활용, 인재 양성, 클라우드 인프라 개선 등이 중점이다.\n",
    "\"\"\"\n",
    "summary_from_text = summarizer.summarize(text)\n",
    "print(\"텍스트 요약 결과:\\n\", summary_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from konlpy.tag import Komoran\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "class KeywordExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "        stop_words: List[str] = None,\n",
    "        ngram_range: Tuple[int, int] = (1, 1),\n",
    "        top_n: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        키워드 추출기 초기화\n",
    "        Args:\n",
    "            model_name: sentence-transformers 모델 이름\n",
    "            stop_words: 불용어 리스트\n",
    "            ngram_range: n-gram 범위\n",
    "            top_n: 상위 N개 키워드 추출\n",
    "        \"\"\"\n",
    "        self.komoran = Komoran()\n",
    "        self.model = KeyBERT(model_name)\n",
    "        self.stop_words = stop_words or ['하는', '있는', '위한', '통한', '되지', '하고']\n",
    "        self.ngram_range = ngram_range\n",
    "        self.top_n = top_n\n",
    "        \n",
    "        # CountVectorizer 사전 초기화\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            tokenizer=self._noun_tokenizer,\n",
    "            ngram_range=ngram_range,\n",
    "            max_df=1.0,\n",
    "            min_df=1\n",
    "        )\n",
    "\n",
    "    def _noun_tokenizer(self, text: str) -> List[str]:\n",
    "        \"\"\"명사 추출 및 2글자 이상 필터링\"\"\"\n",
    "        try:\n",
    "            nouns = self.komoran.nouns(text)\n",
    "            return [noun for noun in nouns if len(noun) >= 2]\n",
    "        except Exception as e:\n",
    "            print(f\"형태소 분석 중 오류 발생: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_keywords(self, text: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        텍스트에서 키워드 추출\n",
    "        Args:\n",
    "            text: 분석할 텍스트\n",
    "        Returns:\n",
    "            (키워드, 점수) 형식의 튜플 리스트\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"입력 텍스트가 비어 있습니다.\")\n",
    "            \n",
    "        keywords = self.model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=self.ngram_range,\n",
    "            stop_words=self.stop_words,\n",
    "            top_n=self.top_n,\n",
    "            vectorizer=self.vectorizer\n",
    "        )\n",
    "        \n",
    "        return self._postprocess(keywords)\n",
    "\n",
    "    def _postprocess(self, keywords: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"추출 결과 후처리\"\"\"\n",
    "        # 숫자로만 구성된 키워드 제거\n",
    "        filtered = [(word, score) for word, score in keywords if not word.isdigit()]\n",
    "        # 중복 제거 (KeyBERT가 종종 중복을 반환하는 경우 처리)\n",
    "        seen = set()\n",
    "        return [(word, score) for word, score in filtered if not (word in seen or seen.add(word))]\n",
    "    \n",
    "    def add_stop_words(self, new_stop_words: List[str]):\n",
    "        \"\"\"불용어 추가\"\"\"\n",
    "        self.stop_words.extend(new_stop_words)\n",
    "        # 벡터라이저 재생성\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            tokenizer=self._noun_tokenizer,\n",
    "            ngram_range=self.ngram_range,\n",
    "            max_df=1.0,\n",
    "            min_df=1,\n",
    "            stop_words=self.stop_words\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용 에시\n",
    "extractor = KeywordExtractor()\n",
    "\n",
    "text = \"\"\"\n",
    "정부는 오늘 새로운 인공지능 산업 전략을 발표했다. 이 전략은 국내 기업의 기술 자립을 촉진하고, \n",
    "글로벌 경쟁력을 확보하기 위한 다양한 정책을 포함하고 있다. 특히 데이터 활용, 인재 양성, 클라우드 인프라 개선 등이 중점이다.\n",
    "\"\"\"\n",
    "\n",
    "keywords = extractor.extract_keywords(text)\n",
    "print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
