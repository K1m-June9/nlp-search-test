# í•œêµ­ì–´ ìš”ì•½ ë°ì´í„°ì…‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œìŠ¤í…œ
# KoreanSummarizeAiHub ë°ì´í„°ì…‹ ì‚¬ìš©

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime
import warnings
import random
import re
from collections import Counter
from tqdm import tqdm
import subprocess
import sys

warnings.filterwarnings('ignore')

def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
    packages = [
        'datasets',
        'keybert', 
        'sentence-transformers',
        'konlpy',
        'networkx',
        'scikit-learn',
        'nltk',
        'scipy'
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ“ {package} ì„¤ì¹˜ ì™„ë£Œ")
        except:
            print(f"âœ— {package} ì„¤ì¹˜ ì‹¤íŒ¨")

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
print("í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")
install_requirements()

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
import networkx as nx
from scipy import stats

# KoNLPy ì„í¬íŠ¸ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
try:
    from konlpy.tag import Okt, Komoran
    KONLPY_AVAILABLE = True
    print("âœ“ KoNLPy ì‚¬ìš© ê°€ëŠ¥")
except:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPy ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©")

class KoreanKeywordDatasetConverter:
    """í•œêµ­ì–´ ìš”ì•½ ë°ì´í„°ì…‹ì„ í‚¤ì›Œë“œ ì¶”ì¶œìš©ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self):
        if KONLPY_AVAILABLE:
            try:
                self.okt = Okt()
                self.use_advanced_tokenizer = True
                print("âœ“ ê³ ê¸‰ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            except:
                self.use_advanced_tokenizer = False
                print("âš ï¸ ê¸°ë³¸ í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´")
        else:
            self.use_advanced_tokenizer = False
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´
        self.stopwords = set([
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜',
            'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”',
            'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ê°™ì€', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ë§ì€', 'ì ì€', 'ìœ„í•œ',
            'í†µí•´', 'ëŒ€í•œ', 'ê´€í•œ', 'ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¡œì„œ', 'ë¶€í„°', 'ê¹Œì§€'
        ])
        
    def extract_nouns_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ"""
        if not text:
            return []
            
        try:
            if self.use_advanced_tokenizer:
                # KoNLPy ì‚¬ìš©
                pos_tags = self.okt.pos(text, stem=True)
                nouns = [word for word, pos in pos_tags 
                        if pos in ['Noun'] and len(word) >= 2]
            else:
                # ê¸°ë³¸ ì •ê·œì‹ ì‚¬ìš©
                nouns = re.findall(r'[ê°€-í£]{2,}', text)
            
            # ë¶ˆìš©ì–´ ì œê±° ë° í•„í„°ë§
            filtered_nouns = [noun for noun in nouns 
                            if noun not in self.stopwords 
                            and len(noun) >= 2 
                            and not noun.isdigit()
                            and len(noun) <= 10]  # ë„ˆë¬´ ê¸´ ë‹¨ì–´ ì œì™¸
            
            return filtered_nouns
            
        except Exception as e:
            print(f"âš ï¸ ëª…ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # ëŒ€ì²´ ë°©ë²•
            words = re.findall(r'[ê°€-í£]{2,}', text)
            return [w for w in words if w not in self.stopwords]
    
    def calculate_keyword_importance(self, nouns, text):
        """ëª…ì‚¬ë“¤ì˜ ì¤‘ìš”ë„ ê³„ì‚°"""
        if not nouns:
            return {}
            
        noun_counts = Counter(nouns)
        text_length = len(text)
        total_nouns = len(nouns)
        
        importance_scores = {}
        
        for noun, count in noun_counts.items():
            # 1. TF ì ìˆ˜ (ë¹ˆë„)
            tf_score = count / total_nouns
            
            # 2. ê¸¸ì´ ë³´ë„ˆìŠ¤ (ê¸´ ëª…ì‚¬ì¼ìˆ˜ë¡ êµ¬ì²´ì )
            length_bonus = min(len(noun) / 8, 0.5)
            
            # 3. ìœ„ì¹˜ ë³´ë„ˆìŠ¤ (ì•ìª½ì— ë‚˜ì˜¬ìˆ˜ë¡ ì¤‘ìš”)
            try:
                first_position = text.find(noun) / text_length
                position_bonus = (1 - first_position) * 0.3
            except:
                position_bonus = 0
            
            # 4. ë¹ˆë„ ë³´ë„ˆìŠ¤ (ì ë‹¹í•œ ë¹ˆë„ê°€ ì¢‹ìŒ)
            frequency_bonus = min(count / 3, 0.3) if count > 1 else 0
            
            # ì´ ì ìˆ˜ ê³„ì‚°
            total_score = tf_score + length_bonus + position_bonus + frequency_bonus
            importance_scores[noun] = total_score
            
        return importance_scores
    
    def extract_reference_keywords(self, passage, summary, method='summary_based'):
        """ì°¸ì¡° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        if method == 'summary_based':
            # ìš”ì•½ë¬¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            summary_nouns = self.extract_nouns_from_text(summary)
            importance_scores = self.calculate_keyword_importance(summary_nouns, summary)
            
        elif method == 'comparison_based':
            # ì›ë¬¸-ìš”ì•½ë¬¸ ë¹„êµ ê¸°ë°˜
            passage_nouns = set(self.extract_nouns_from_text(passage))
            summary_nouns = self.extract_nouns_from_text(summary)
            
            # ìš”ì•½ë¬¸ì— ìˆìœ¼ë©´ì„œ ì›ë¬¸ì—ë„ ìˆëŠ” ëª…ì‚¬ë“¤ (ì¤‘ìš”í•œ í‚¤ì›Œë“œì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
            common_nouns = [noun for noun in summary_nouns if noun in passage_nouns]
            
            if common_nouns:
                importance_scores = self.calculate_keyword_importance(common_nouns, summary)
            else:
                # ê³µí†µ ëª…ì‚¬ê°€ ì—†ìœ¼ë©´ ìš”ì•½ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
                importance_scores = self.calculate_keyword_importance(summary_nouns, summary)
                
        elif method == 'hybrid':
            # í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•
            summary_nouns = self.extract_nouns_from_text(summary)
            passage_nouns = set(self.extract_nouns_from_text(passage))
            
            # ìš”ì•½ë¬¸ ëª…ì‚¬ë“¤ì˜ ì¤‘ìš”ë„ ê³„ì‚°
            summary_importance = self.calculate_keyword_importance(summary_nouns, summary)
            
            # ì›ë¬¸ì—ë„ ìˆëŠ” ëª…ì‚¬ë“¤ì—ê²Œ ë³´ë„ˆìŠ¤ ì ìˆ˜
            importance_scores = {}
            for noun, score in summary_importance.items():
                bonus = 0.5 if noun in passage_nouns else 0
                importance_scores[noun] = score + bonus
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        if not importance_scores:
            return []
            
        sorted_keywords = sorted(importance_scores.items(), 
                               key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜ (ì¤‘ë³µ ì œê±°)
        keywords = []
        seen = set()
        for keyword, score in sorted_keywords:
            if keyword.lower() not in seen and len(keywords) < 15:  # ë” ë§ì´ ìƒì„±
                keywords.append(keyword)
                seen.add(keyword.lower())
                
        return keywords
    
    def convert_dataset(self, dataset, num_samples=100, keyword_method='summary_based'):
        """ë°ì´í„°ì…‹ ë³€í™˜"""
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë³€í™˜ ì¤‘... (ë°©ë²•: {keyword_method})")
        
        converted_data = []
        successful_conversions = 0
        
        for i, item in enumerate(dataset[:num_samples * 2]):  # ì—¬ìœ ìˆê²Œ ë” ë§ì´ ì‹œë„
            try:
                passage = item['passage']
                summary = item['summary']
                
                # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸
                if len(passage) < 50 or len(summary) < 10:
                    continue
                
                # ì°¸ì¡° í‚¤ì›Œë“œ ìƒì„±
                reference_keywords = self.extract_reference_keywords(
                    passage, summary, method=keyword_method
                )
                
                # í‚¤ì›Œë“œê°€ ì¶©ë¶„íˆ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if len(reference_keywords) >= 3:
                    # ì›ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    converted_data.append({
                        'text': passage,
                        'keywords': reference_keywords,
                        'summary': summary,
                        'data_type': 'passage',
                        'original_index': i
                    })
                    
                    # ìš”ì•½ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    converted_data.append({
                        'text': summary,
                        'keywords': reference_keywords,
                        'passage': passage,
                        'data_type': 'summary', 
                        'original_index': i
                    })
                    
                    successful_conversions += 1
                    
                    if (successful_conversions) % 10 == 0:
                        print(f"  - {successful_conversions}ê°œ ë³€í™˜ ì™„ë£Œ")
                        print(f"    ì˜ˆì‹œ í‚¤ì›Œë“œ: {reference_keywords[:5]}")
                
                # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                if successful_conversions >= num_samples:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ ìƒ˜í”Œ {i} ë³€í™˜ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… ì´ {len(converted_data)}ê°œ ë°ì´í„° ë³€í™˜ ì™„ë£Œ (ì›ë³¸ {successful_conversions}ê°œ)")
        return converted_data

class KoreanKeywordExtractorComparison:
    """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ë¹„êµ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, num_samples=50, random_seed=42):
        self.num_samples = num_samples
        self.random_seed = random_seed
        
        # ëœë¤ ì‹œë“œ ì„¤ì •
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = {}
        self.predictions = {}
        self.references = []
        self.test_texts = []
        
        print(f"ğŸš€ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì‹œë“œ: {random_seed})")
        
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        print("\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # KeyBERT (í•œêµ­ì–´ íŠ¹í™”)
        print("  - KeyBERT ë¡œë”©...")
        try:
            self.keybert = KeyBERT('klue/bert-base')
            print("    âœ“ KLUE BERT ëª¨ë¸ ì‚¬ìš©")
        except:
            self.keybert = KeyBERT('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            print("    âœ“ Multilingual ëª¨ë¸ ì‚¬ìš©")
        
        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ìš©
        print("  - Sentence-BERT ë¡œë”©...")
        try:
            self.semantic_model = SentenceTransformer('klue/bert-base')
        except:
            self.semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
        if KONLPY_AVAILABLE:
            try:
                self.okt = Okt()
                self.use_advanced_tokenizer = True
            except:
                self.use_advanced_tokenizer = False
        else:
            self.use_advanced_tokenizer = False
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´
        self.korean_stopwords = set([
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ë°', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜',
            'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”',
            'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ê°™ì€', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ë§ì€', 'ì ì€'
        ])
        
        # TF-IDFëŠ” ë‚˜ì¤‘ì— í•œêµ­ì–´ ë°ì´í„°ë¡œ í•™ìŠµ
        self.tfidf_vectorizer = None
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
    def _korean_tokenizer(self, text):
        """í•œêµ­ì–´ í† í¬ë‚˜ì´ì €"""
        if not text:
            return []
            
        try:
            if self.use_advanced_tokenizer:
                pos_tags = self.okt.pos(text, stem=True)
                keywords = [word for word, pos in pos_tags 
                           if pos in ['Noun', 'Adjective'] and len(word) >= 2]
            else:
                keywords = re.findall(r'[ê°€-í£]{2,}', text)
            
            return [kw for kw in keywords if kw not in self.korean_stopwords]
            
        except:
            words = re.findall(r'[ê°€-í£]{2,}', text)
            return [w for w in words if w not in self.korean_stopwords]
    
    def _preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _post_process_keywords(self, keywords, top_k=10):
        """í‚¤ì›Œë“œ í›„ì²˜ë¦¬"""
        if not keywords:
            return []
        
        # íŠœí”Œì—ì„œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        processed = []
        for kw in keywords:
            if isinstance(kw, tuple):
                processed.append(str(kw[0]))
            else:
                processed.append(str(kw))
        
        # í•„í„°ë§
        filtered = []
        for kw in processed:
            kw = kw.strip()
            if 2 <= len(kw) <= 15 and not kw.isdigit():
                if re.search(r'[ê°€-í£]', kw):  # í•œêµ­ì–´ í¬í•¨
                    filtered.append(kw)
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_keywords = []
        for kw in filtered:
            kw_lower = kw.lower()
            if kw_lower not in seen:
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        return unique_keywords[:top_k]
    
    def load_korean_dataset(self):
        """í•œêµ­ì–´ ìš”ì•½ ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜"""
        print(f"\nğŸ“Š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        try:
            # í•œêµ­ì–´ ìš”ì•½ ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = load_dataset("Laplace04/KoreanSummarizeAiHub")
            test_data = dataset['test']
            print(f"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(test_data)}")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œìš©ìœ¼ë¡œ ë³€í™˜
            converter = KoreanKeywordDatasetConverter()
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë³€í™˜ (ê°€ì¥ ì¢‹ì€ ë°©ë²• ì„ íƒ)
            print("\në‹¤ì–‘í•œ í‚¤ì›Œë“œ ìƒì„± ë°©ë²• í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            methods = ['summary_based', 'comparison_based', 'hybrid']
            best_method = 'hybrid'  # ê¸°ë³¸ê°’
            best_quality = 0
            
            for method in methods:
                print(f"\n--- {method} ë°©ë²• í…ŒìŠ¤íŠ¸ ---")
                sample_data = converter.convert_dataset(
                    test_data, num_samples=10, keyword_method=method
                )
                
                # í’ˆì§ˆ í‰ê°€ (í‚¤ì›Œë“œ ê°œìˆ˜ì™€ ë‹¤ì–‘ì„±)
                if sample_data:
                    avg_keywords = np.mean([len(item['keywords']) for item in sample_data])
                    keyword_diversity = len(set([kw for item in sample_data 
                                               for kw in item['keywords']])) / max(1, len([kw for item in sample_data for kw in item['keywords']]))
                    quality_score = avg_keywords * keyword_diversity
                    
                    print(f"  í‰ê·  í‚¤ì›Œë“œ ìˆ˜: {avg_keywords:.1f}")
                    print(f"  í‚¤ì›Œë“œ ë‹¤ì–‘ì„±: {keyword_diversity:.3f}")
                    print(f"  í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
                    
                    if quality_score > best_quality:
                        best_quality = quality_score
                        best_method = method
            
            print(f"\nâœ… ìµœì  ë°©ë²• ì„ íƒ: {best_method}")
            
            # ìµœì  ë°©ë²•ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ë³€í™˜
            converted_data = converter.convert_dataset(
                test_data, num_samples=self.num_samples, keyword_method=best_method
            )
            
            # ì›ë¬¸ê³¼ ìš”ì•½ë¬¸ ë°ì´í„° ë¶„ë¦¬
            passage_data = [item for item in converted_data if item['data_type'] == 'passage']
            summary_data = [item for item in converted_data if item['data_type'] == 'summary']
            
            # ìš”ì•½ë¬¸ ë°ì´í„° ì‚¬ìš© (í‚¤ì›Œë“œ ì¶”ì¶œì— ë” ì í•©)
            self.test_data = summary_data[:self.num_samples]
            self.test_texts = [item['text'] for item in self.test_data]
            self.references = [item['keywords'] for item in self.test_data]
            
            print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.test_data)}ê°œ")
            
            # ìƒ˜í”Œ ì¶œë ¥
            if self.test_data:
                print(f"\nğŸ“ ìƒ˜í”Œ ì˜ˆì‹œ:")
                sample = self.test_data[0]
                print(f"í…ìŠ¤íŠ¸: {sample['text'][:100]}...")
                print(f"í‚¤ì›Œë“œ: {sample['keywords'][:5]}")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.test_data = []
    
    def prepare_korean_tfidf(self):
        """í•œêµ­ì–´ TF-IDF ì¤€ë¹„"""
        print("\nâš–ï¸ í•œêµ­ì–´ TF-IDF ì¤€ë¹„ ì¤‘...")
        
        try:
            # í˜„ì¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš© (êµì°¨ ê²€ì¦ ë°©ì‹)
            if len(self.test_data) > 20:
                # ì•ì˜ 20%ëŠ” TF-IDF í•™ìŠµìš©, ë‚˜ë¨¸ì§€ëŠ” í…ŒìŠ¤íŠ¸ìš©
                split_idx = len(self.test_data) // 5
                train_texts = [item['text'] for item in self.test_data[:split_idx]]
                
                # ì¶”ê°€ë¡œ ìš”ì•½ë¬¸ë“¤ë„ í•™ìŠµ ë°ì´í„°ì— í¬í•¨
                if 'passage' in self.test_data[0]:
                    train_texts.extend([item['passage'] for item in self.test_data[:split_idx] 
                                      if 'passage' in item])
                
                print(f"  TF-IDF í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ")
            else:
                # ë°ì´í„°ê°€ ì ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµ
                train_texts = self.test_texts
                print(f"  TF-IDF í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ (ì „ì²´)")
            
            # í•œêµ­ì–´ íŠ¹í™” TF-IDF ë²¡í„°ë¼ì´ì €
            self.tfidf_vectorizer = TfidfVectorizer(
                tokenizer=self._korean_tokenizer,
                ngram_range=(1, 2),
                max_features=5000,
                min_df=1,
                max_df=0.9,
                lowercase=False,  # í•œêµ­ì–´ëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
                sublinear_tf=True
            )
            
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµ
            self.tfidf_vectorizer.fit(train_texts)
            print("âœ… í•œêµ­ì–´ TF-IDF í•™ìŠµ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âš ï¸ TF-IDF ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            # ìµœì†Œí•œì˜ ë²¡í„°ë¼ì´ì €
            self.tfidf_vectorizer = TfidfVectorizer(
                ngram_range=(1, 1),
                max_features=1000,
                min_df=1
            )
            basic_texts = ["í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"] * 5
            self.tfidf_vectorizer.fit(basic_texts)
            print("âœ… ê¸°ë³¸ TF-IDF í•™ìŠµ ì™„ë£Œ!")
    
    def extract_keybert_keywords(self, text, top_k=10):
        """KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not text.strip():
                return []
            
            keywords = self.keybert.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 2),
                stop_words=None,
                top_n=top_k * 2,
                use_mmr=True,
                diversity=0.5,
                use_maxsum=True,
                nr_candidates=20
            )
            
            keyword_list = [kw[0] for kw in keywords]
            return self._post_process_keywords(keyword_list, top_k)
            
        except Exception as e:
            print(f"âš ï¸ KeyBERT ì˜¤ë¥˜: {e}")
            return []
    
    def extract_tfidf_keywords(self, text, top_k=10):
        """TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not text.strip() or self.tfidf_vectorizer is None:
                return []
            
            tfidf_vector = self.tfidf_vectorizer.transform([text])
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            scores = tfidf_vector.toarray()[0]
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            top_indices = scores.argsort()[-(top_k * 2):][::-1]
            keywords = [feature_names[i] for i in top_indices if scores[i] > 0]
            
            return self._post_process_keywords(keywords, top_k)
            
        except Exception as e:
            print(f"âš ï¸ TF-IDF ì˜¤ë¥˜: {e}")
            return []
    
    def extract_textrank_keywords(self, text, top_k=10):
        """TextRank í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not text.strip():
                return []
            
            tokens = self._korean_tokenizer(text)
            
            if len(tokens) < 3:
                return self._post_process_keywords(tokens, top_k)
            
            # ê·¸ë˜í”„ ìƒì„±
            graph = nx.Graph()
            window_size = min(5, max(3, len(tokens) // 8))
            
            for i in range(len(tokens) - window_size + 1):
                window = tokens[i:i + window_size]
                for j in range(len(window)):
                    for k in range(j + 1, len(window)):
                        if window[j] != window[k]:
                            distance_weight = 1.0 / (abs(j - k) + 1)
                            
                            if graph.has_edge(window[j], window[k]):
                                graph[window[j]][window[k]]['weight'] += distance_weight
                            else:
                                graph.add_edge(window[j], window[k], weight=distance_weight)
            
            if len(graph.nodes()) == 0:
                return self._post_process_keywords(tokens, top_k)
            
            pagerank_scores = nx.pagerank(graph, weight='weight', alpha=0.85)
            sorted_keywords = sorted(pagerank_scores.items(), 
                                   key=lambda x: x[1], reverse=True)
            
            keywords = [kw[0] for kw in sorted_keywords]
            return self._post_process_keywords(keywords, top_k)
            
        except Exception as e:
            print(f"âš ï¸ TextRank ì˜¤ë¥˜: {e}")
            return []
    
    def extract_hybrid_keywords(self, text, top_k=10):
        """í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not text.strip():
                return []
            
            # ê° ë°©ë²•ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            textrank_keywords = self.extract_textrank_keywords(text, top_k * 2)
            keybert_keywords = self.extract_keybert_keywords(text, top_k * 2)
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
            keyword_scores = {}
            
            # TextRank ì ìˆ˜ (êµ¬ì¡°ì  ì¤‘ìš”ë„)
            for i, kw in enumerate(textrank_keywords):
                score = (len(textrank_keywords) - i) / len(textrank_keywords) if textrank_keywords else 0
                keyword_scores[kw] = keyword_scores.get(kw, 0) + score * 0.4
            
            # KeyBERT ì ìˆ˜ (ì˜ë¯¸ì  ì¤‘ìš”ë„)
            for i, kw in enumerate(keybert_keywords):
                score = (len(keybert_keywords) - i) / len(keybert_keywords) if keybert_keywords else 0
                keyword_scores[kw] = keyword_scores.get(kw, 0) + score * 0.6
            
            # ê³µí†µ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
            common_keywords = set(textrank_keywords) & set(keybert_keywords)
            for kw in common_keywords:
                keyword_scores[kw] = keyword_scores.get(kw, 0) + 0.3
            
            # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
            if keyword_scores:
                sorted_keywords = sorted(keyword_scores.items(), 
                                       key=lambda x: x[1], reverse=True)
                hybrid_keywords = [kw[0] for kw in sorted_keywords]
                return self._post_process_keywords(hybrid_keywords, top_k)
            else:
                return []
                
        except Exception as e:
            print(f"âš ï¸ Hybrid ì˜¤ë¥˜: {e}")
            return self.extract_keybert_keywords(text, top_k)
    
    def extract_all_keywords(self):
        """ëª¨ë“  ë°©ë²•ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print(f"\nğŸ”„ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ ({len(self.test_data)}ê°œ ìƒ˜í”Œ)...")
        
        keybert_predictions = []
        tfidf_predictions = []
        textrank_predictions = []
        hybrid_predictions = []
        
        for i, item in enumerate(tqdm(self.test_data, desc="í‚¤ì›Œë“œ ì¶”ì¶œ")):
            text = item['text']
            
            # ê° ë°©ë²•ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keybert_kw = self.extract_keybert_keywords(text, top_k=10)
            tfidf_kw = self.extract_tfidf_keywords(text, top_k=10)
            textrank_kw = self.extract_textrank_keywords(text, top_k=10)
            hybrid_kw = self.extract_hybrid_keywords(text, top_k=10)
            
            keybert_predictions.append(keybert_kw)
            tfidf_predictions.append(tfidf_kw)
            textrank_predictions.append(textrank_kw)
            hybrid_predictions.append(hybrid_kw)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if (i + 1) % 10 == 0:
                print(f"\n--- ìƒ˜í”Œ {i+1} ---")
                print(f"í…ìŠ¤íŠ¸: {text[:80]}...")
                print(f"ì°¸ì¡°: {item['keywords'][:3]}")
                print(f"KeyBERT: {keybert_kw[:3]}")
                print(f"TF-IDF: {tfidf_kw[:3]}")
                print(f"TextRank: {textrank_kw[:3]}")
                print(f"Hybrid: {hybrid_kw[:3]}")
        
        # ê²°ê³¼ ì €ì¥
        self.predictions = {
            'keybert': keybert_predictions,
            'tfidf': tfidf_predictions,
            'textrank': textrank_predictions,
            'hybrid': hybrid_predictions
        }
        
        print("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
    
    def calculate_precision_recall_f1(self, predicted, true, k=5):
        """Precision@K, Recall@K, F1@K ê³„ì‚°"""
        pred_k = set([kw.lower().strip() for kw in predicted[:k] if kw])
        true_set = set([kw.lower().strip() for kw in true if kw])
        
        if len(true_set) == 0 or len(pred_k) == 0:
            return {"precision": 0, "recall": 0, "f1": 0}
        
        intersection = pred_k.intersection(true_set)
        
        precision = len(intersection) / len(pred_k) if len(pred_k) > 0 else 0
        recall = len(intersection) / len(true_set)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return {"precision": precision, "recall": recall, "f1": f1}
    
    def calculate_partial_match_score(self, predicted, true, k=5):
        """ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        pred_k = [kw.lower().strip() for kw in predicted[:k] if kw]
        true_list = [kw.lower().strip() for kw in true if kw]
        
        if len(true_list) == 0 or len(pred_k) == 0:
            return 0
        
        matches = 0
        for pred in pred_k:
            for true_kw in true_list:
                if (pred == true_kw or 
                    pred in true_kw or 
                    true_kw in pred or
                    self._fuzzy_match(pred, true_kw)):
                    matches += 1
                    break
        
        return matches / len(pred_k) if len(pred_k) > 0 else 0
    
    def _fuzzy_match(self, str1, str2, threshold=0.7):
        """ìœ ì‚¬ ë§¤ì¹­"""
        if len(str1) < 2 or len(str2) < 2:
            return False
        
        set1 = set(str1)
        set2 = set(str2)
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return (intersection / union) > threshold if union > 0 else False
    
    def calculate_semantic_similarity(self, predicted, true, k=5):
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        pred_k = [kw for kw in predicted[:k] if kw.strip()]
        true_list = [kw for kw in true if kw.strip()]
        
        if len(true_list) == 0 or len(pred_k) == 0:
            return 0
        
        try:
            pred_embeddings = self.semantic_model.encode(pred_k)
            true_embeddings = self.semantic_model.encode(true_list)
            
            similarities = cosine_similarity(pred_embeddings, true_embeddings)
            max_similarities = np.max(similarities, axis=1)
            
            return np.mean(max_similarities)
            
        except:
            return 0
    
    def calculate_all_metrics(self):
        """ëª¨ë“  í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        print("\nğŸ“Š í‰ê°€ ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        methods = ['keybert', 'tfidf', 'textrank', 'hybrid']
        metrics = ['precision', 'recall', 'f1', 'partial_match', 'semantic_sim']
        
        self.results = {}
        
        for method in methods:
            print(f"  - {method.upper()} í‰ê°€ ì¤‘...")
            
            self.results[method] = {metric: [] for metric in metrics}
            predictions = self.predictions[method]
            
            for pred, true in zip(predictions, self.references):
                # ê¸°ë³¸ ì§€í‘œ
                basic_metrics = self.calculate_precision_recall_f1(pred, true, k=5)
                self.results[method]['precision'].append(basic_metrics['precision'])
                self.results[method]['recall'].append(basic_metrics['recall'])
                self.results[method]['f1'].append(basic_metrics['f1'])
                
                # ë¶€ë¶„ ë§¤ì¹­
                partial_score = self.calculate_partial_match_score(pred, true, k=5)
                self.results[method]['partial_match'].append(partial_score)
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„
                sem_sim = self.calculate_semantic_similarity(pred, true, k=5)
                self.results[method]['semantic_sim'].append(sem_sim)
        
        print("âœ… ëª¨ë“  í‰ê°€ ì§€í‘œ ê³„ì‚° ì™„ë£Œ!")
    
    def create_visualizations(self):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        methods = ['keybert', 'tfidf', 'textrank', 'hybrid']
        metrics = ['precision', 'recall', 'f1', 'partial_match', 'semantic_sim']
        
        # 1. ë°•ìŠ¤í”Œë¡¯
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Korean Keyword Extraction Performance Distribution', fontsize=16, fontweight='bold')
        
        for i, metric in enumerate(metrics):
            row = i // 3
            col = i % 3
            
            data_to_plot = [self.results[method][metric] for method in methods]
            
            axes[row, col].boxplot(data_to_plot, labels=['KeyBERT', 'TF-IDF', 'TextRank', 'Hybrid'])
            axes[row, col].set_title(f'{metric.upper()}@5')
            axes[row, col].grid(True, alpha=0.3)
            axes[row, col].tick_params(axis='x', rotation=45)
        
        # ë¹ˆ subplot ì œê±°
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig('korean_keyword_extraction_boxplot.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. í‰ê·  ì„±ëŠ¥ ë¹„êµ
        fig, ax = plt.subplots(figsize=(14, 8))
        
        x = np.arange(len(metrics))
        width = 0.2
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
        
        for i, method in enumerate(methods):
            means = [np.mean(self.results[method][metric]) for metric in metrics]
            stds = [np.std(self.results[method][metric]) for metric in metrics]
            
            ax.bar(x + i*width, means, width, yerr=stds, 
                   label=method.upper(), alpha=0.8, capsize=5, color=colors[i])
        
        ax.set_xlabel('Metrics')
        ax.set_ylabel('Scores')
        ax.set_title('Korean Keyword Extraction Performance Comparison')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels([m.upper() for m in metrics])
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('korean_keyword_extraction_barplot.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 3. íˆíŠ¸ë§µ
        heatmap_data = []
        for method in methods:
            row = [np.mean(self.results[method][metric]) for metric in metrics]
            heatmap_data.append(row)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        sns.heatmap(heatmap_data,
                   xticklabels=[m.upper() for m in metrics],
                   yticklabels=[m.upper() for m in methods],
                   annot=True, fmt='.3f', cmap='YlOrRd',
                   ax=ax)
        ax.set_title('Korean Keyword Extraction Performance Heatmap')
        plt.tight_layout()
        plt.savefig('korean_keyword_extraction_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ì‹œê°í™” ì™„ë£Œ!")
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ìƒì„¸ ê²°ê³¼ CSV
        detailed_results = []
        for i in range(len(self.references)):
            row = {
                'sample_id': i,
                'text': self.test_texts[i][:200] + "..." if len(self.test_texts[i]) > 200 else self.test_texts[i],
                'true_keywords': '; '.join(self.references[i][:10])
            }
            
            for method in ['keybert', 'tfidf', 'textrank', 'hybrid']:
                row[f'{method}_keywords'] = '; '.join(self.predictions[method][i][:5])
                
                for metric in ['precision', 'recall', 'f1', 'partial_match', 'semantic_sim']:
                    row[f'{method}_{metric}'] = self.results[method][metric][i]
            
            detailed_results.append(row)
        
        df = pd.DataFrame(detailed_results)
        df.to_csv('korean_keyword_extraction_results.csv', index=False, encoding='utf-8-sig')
        
        # ìš”ì•½ ê²°ê³¼ JSON
        summary_data = {
            'experiment_info': {
                'num_samples': self.num_samples,
                'random_seed': self.random_seed,
                'evaluation_date': datetime.now().isoformat(),
                'dataset': 'KoreanSummarizeAiHub',
                'language': 'Korean',
                'methods': ['KeyBERT', 'TF-IDF', 'TextRank', 'Hybrid']
            },
            'performance_summary': {},
            'sample_predictions': {
                'keybert': self.predictions['keybert'][:3],
                'tfidf': self.predictions['tfidf'][:3],
                'textrank': self.predictions['textrank'][:3],
                'hybrid': self.predictions['hybrid'][:3],
                'references': self.references[:3]
            }
        }
        
        # ì„±ëŠ¥ ìš”ì•½ ê³„ì‚°
        for method in ['keybert', 'tfidf', 'textrank', 'hybrid']:
            summary_data['performance_summary'][method] = {}
            for metric in ['precision', 'recall', 'f1', 'partial_match', 'semantic_sim']:
                scores = self.results[method][metric]
                summary_data['performance_summary'][method][metric] = {
                    'mean': float(np.mean(scores)),
                    'std': float(np.std(scores)),
                    'median': float(np.median(scores))
                }
        
        with open('korean_keyword_extraction_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
        print("  - korean_keyword_extraction_results.csv: ìƒì„¸ ê²°ê³¼")
        print("  - korean_keyword_extraction_summary.json: í†µê³„ ìš”ì•½")
    
    def generate_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        methods = ['keybert', 'tfidf', 'textrank', 'hybrid']
        metrics = ['precision', 'recall', 'f1', 'partial_match', 'semantic_sim']
        
        report = []
        report.append("=" * 80)
        report.append("í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¹„êµ í‰ê°€ ë³´ê³ ì„œ")
        report.append("ë°ì´í„°ì…‹: KoreanSummarizeAiHub")
        report.append("=" * 80)
        report.append(f"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {self.num_samples}")
        report.append(f"ì–¸ì–´: í•œêµ­ì–´")
        report.append("")
        
        # ì„±ëŠ¥ ìš”ì•½
        report.append("ğŸ“Š ì„±ëŠ¥ ìš”ì•½ (í‰ê·  Â± í‘œì¤€í¸ì°¨)")
        report.append("-" * 60)
        
        for metric in metrics:
            report.append(f"\n{metric.upper()}:")
            for method in methods:
                mean_score = np.mean(self.results[method][metric])
                std_score = np.std(self.results[method][metric])
                report.append(f"  {method.upper():>10}: {mean_score:.4f} Â± {std_score:.4f}")
        
        report.append("")
        
        # ìµœê³  ì„±ëŠ¥ ë°©ë²•
        report.append("ğŸ† ìµœê³  ì„±ëŠ¥ ë°©ë²•")
        report.append("-" * 40)
        
        for metric in metrics:
            best_method = max(methods, key=lambda m: np.mean(self.results[m][metric]))
            best_score = np.mean(self.results[best_method][metric])
            report.append(f"{metric.upper():>15}: {best_method.upper()} ({best_score:.4f})")
        
        report.append("")
        
        # ì „ì²´ ìŠ¹ìˆ˜ ê³„ì‚°
        wins = {method: 0 for method in methods}
        for metric in metrics:
            best_method = max(methods, key=lambda m: np.mean(self.results[m][metric]))
            wins[best_method] += 1
        
        best_overall = max(wins, key=wins.get)
        report.append("ğŸ¯ ì¢…í•© ê²°ë¡ ")
        report.append("-" * 40)
        report.append(f"â€¢ ì „ë°˜ì  ìµœê³  ì„±ëŠ¥: {best_overall.upper()} ({wins[best_overall]}/{len(metrics)} ì§€í‘œì—ì„œ 1ìœ„)")
        
        # ì‹¤ìš©ì  ê¶Œì¥ì‚¬í•­
        report.append("")
        report.append("ğŸ’¡ ì‹¤ìš©ì  ê¶Œì¥ì‚¬í•­")
        report.append("-" * 40)
        report.append("â€¢ í•œêµ­ì–´ í™˜ê²½ì—ì„œëŠ” ì–¸ì–´ íŠ¹í™” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¤‘ìš”")
        report.append("â€¢ ìš”ì•½ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ í•µì‹¬ ì§€í‘œ")
        report.append("â€¢ í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´ ê°€ëŠ¥")
        report.append("â€¢ ê³µê³µê¸°ê´€ ë¬¸ì„œì—ì„œëŠ” ë„ë©”ì¸ íŠ¹í™” í›„ì²˜ë¦¬ í•„ìš”")
        
        report.append("")
        report.append("=" * 80)
        
        # ë³´ê³ ì„œ ì €ì¥ ë° ì¶œë ¥
        with open('korean_keyword_extraction_report.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        for line in report:
            print(line)
        
        print("\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ! (korean_keyword_extraction_report.txt)")
    
    def run_korean_evaluation(self):
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ í‰ê°€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œì‘!")
        print("=" * 60)
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            self.load_models()
            
            # 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ
            self.load_korean_dataset()
            
            if not self.test_data:
                print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 3. í•œêµ­ì–´ TF-IDF ì¤€ë¹„
            self.prepare_korean_tfidf()
            
            # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
            self.extract_all_keywords()
            
            # 5. í‰ê°€ ì§€í‘œ ê³„ì‚°
            self.calculate_all_metrics()
            
            # 6. ì‹œê°í™”
            self.create_visualizations()
            
            # 7. ê²°ê³¼ ì €ì¥
            self.save_results()
            
            # 8. ë³´ê³ ì„œ ìƒì„±
            self.generate_report()
            
            print("\nğŸ‰ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ í‰ê°€ ì™„ë£Œ!")
            print("ìƒì„±ëœ íŒŒì¼ë“¤:")
            print("  - korean_keyword_extraction_results.csv")
            print("  - korean_keyword_extraction_summary.json")
            print("  - korean_keyword_extraction_report.txt")
            print("  - korean_keyword_extraction_*.png")
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

evaluator = KoreanKeywordExtractorComparison(num_samples=50, random_seed=42)
evaluator.run_korean_evaluation()