{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 자료 문제 해결을 위한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 및 모델 로드\n",
    "import torch\n",
    "import re\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "# KoBART 모델 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 전처리 강화\n",
    "def chunk_text(text, sentence_per_chunk=10):\n",
    "    # 정규표현식으로 문장 단위 분리 (.!? 뒤 공백이 올 경우 분리)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  \n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for idx, sent in enumerate(sentences):\n",
    "        current_chunk.append(sent)\n",
    "        # 10문장 단위로 청크 생성 또는 마지막 남은 문장 처리\n",
    "        if (idx + 1) % sentence_per_chunk == 0 or idx == len(sentences)-1:\n",
    "            chunk = ' '.join(current_chunk).strip()\n",
    "            if chunk:  # 빈 문자열 체크\n",
    "                chunks.append(chunk)\n",
    "            current_chunk = []\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1번\n",
    "#모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=8,                # 빔 서치 확장\n",
    "        max_length=300,             # 출력 길이 조정\n",
    "        min_length=100,\n",
    "        repetition_penalty=2.5,     # 반복 억제\n",
    "        length_penalty=0.9,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2번\n",
    "# 모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=9,                # 빔 서치 확장\n",
    "        max_length=200,             # 출력 길이 조정\n",
    "        min_length=100,\n",
    "        repetition_penalty=3.2,     # 반복 억제\n",
    "        length_penalty=0.9,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=4      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3번\n",
    "# 모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=7,                # 빔 서치 확장\n",
    "        max_length=150,             # 출력 길이 조정\n",
    "        min_length=100,\n",
    "        repetition_penalty=2.8,     # 반복 억제\n",
    "        length_penalty=0.8,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4번\n",
    "# 모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=7,                # 빔 서치 확장\n",
    "        max_length=150,             # 출력 길이 조정\n",
    "        min_length=100,\n",
    "        repetition_penalty=3.0,     # 반복 억제\n",
    "        length_penalty=0.8,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=4      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5번\n",
    "# 모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=7,                # 빔 서치 확장\n",
    "        max_length=120,             # 출력 길이 조정\n",
    "        min_length=75,\n",
    "        repetition_penalty=3.0,     # 반복 억제\n",
    "        length_penalty=0.8,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=4      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6번\n",
    "# 모델 하이퍼파라미터 최적화\n",
    "def summarize_text(text):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=1024  # 모델 최대 입력 길이 확인\n",
    "    )\n",
    "    \n",
    "    input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=6,                # 빔 서치 확장\n",
    "        max_length=120,             # 출력 길이 조정\n",
    "        min_length=75,\n",
    "        repetition_penalty=3.0,     # 반복 억제\n",
    "        length_penalty=0.8,         # 길이 패널티\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=4      # 중복 n-gram 제한\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#계층적 요약 구조 도입\n",
    "\"\"\"\n",
    "✓ 긴 문서(3,000자+) → depth 3-4 사용시 계층적 추상화 가능\n",
    "✓ 짧은 문서(~500자) → depth 1 고정 권장\n",
    "✓ 중간 길이 문서 → depth 2가 최적의 밸런스\n",
    "✓ 품질 저하 발생시 → 동적 depth 조정이나 하이브리드 기법 도입\n",
    "\"\"\"\n",
    "def hierarchical_summarization(text, depth=2):\n",
    "    if depth == 0:  # 최종 단계\n",
    "        return summarize_text(text)\n",
    "    \n",
    "    # 현재 단계에서 청크 분할\n",
    "    chunks = chunk_text(text) \n",
    "    # 각 청크를 하위 단계로 전달 (재귀)\n",
    "    summaries = [hierarchical_summarization(chunk, depth-1) for chunk in chunks]\n",
    "    # 상위 요약 수행\n",
    "    return summarize_text(' '.join(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 전처리 강화\n",
    "def preprocess_text(text):\n",
    "    \"\"\"요약 전 텍스트 정제\"\"\"\n",
    "    # 연속 공백 단일화\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    # [각주1] 형태 제거\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  \n",
    "    # 1.2.3 같은 번호 체계 제거\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  \n",
    "    # 특수문자 정규화\n",
    "    text = re.sub(r'[●◆▶▼]+', '', text)  \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 요약 결과 후처리(미정)\n",
    "# 요약 결과 후처리\n",
    "def postprocess_summary(summary):\n",
    "    \"\"\"중복 문장 제거 및 결과 정제\"\"\"\n",
    "    sentences = list(dict.fromkeys(summary.split('. ')))  # 중복 제거\n",
    "    return '. '.join(sentences).replace(' ,', ',')  # 문법 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 요약 결과:\n",
      "중앙기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후 성별영향평가서를 작성하여 각 기관에 제출할 예정이며  (성별ᆞ생년월일, 법인의 경우는 그 명칭 및 대표자 성명)\\n전자우편\\n전화번호\\n내용\\n휴대전화 번호\\n신청\\n1사용목적\\n2사용대수\\n3사용기간\\n1일 사용 신청서\\n신.\n"
     ]
    }
   ],
   "source": [
    "#문서 텍스트 추출\n",
    "import fitz\n",
    "file_path = './example.pdf'\n",
    "ex_pdf = []\n",
    "doc = fitz.open(file_path)\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    ex_pdf.append(text)\n",
    "    \n",
    "\n",
    "# 1. PDF 텍스트 추출\n",
    "ex_pdf = str(ex_pdf).replace('·','\\t')#PDF에서 추출한 원본 텍스트\n",
    "\n",
    "# 2. 전처리\n",
    "cleaned_text = preprocess_text(ex_pdf)\n",
    "\n",
    "# 3. 계층적 요약 (2단계)\n",
    "raw_summary = hierarchical_summarization(cleaned_text, depth=2)\n",
    "\n",
    "# 4. 후처리\n",
    "final_summary = postprocess_summary(raw_summary)\n",
    "\n",
    "print(\"최종 요약 결과:\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중앙부처기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후 성별영향평가서'를 작성하여 각 기관에 제출할 예정이며, ‘성별ᆞ생년월일’은 법인의 경우 법인등록번호) 주소(법인의 경우는 주된 사무소의 소재지)  전자우편 전화번호 신청 내용 전화, 전화번호, 홈페이지 등을 통해 신청을 받는다.\n"
     ]
    }
   ],
   "source": [
    "result_1 = \"\"\"중앙부처기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후 성별영향평가서'를 작성하여 각 기관에 제출할 예정이며, \n",
    "‘성별ᆞ생년월일’은 법인의 경우 법인등록번호)\\n주소(법인의 경우는 주된 사무소의 소재지) \\n전자우편\\n전화번호\\n신청\\n내용\\n전화, 전화번호, 홈페이지 등을 통해 신청을 받는다.\"\"\"\n",
    "\n",
    "result_2 = \"\"\"\n",
    "중앙기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후 성별영향평가서를 작성하여 각 기관에 제출할 예정이며  \n",
    "(성별ᆞ생년월일, 법인의 경우는 그 명칭 및 대표자 성명)\\n전자우편\\n전화번호\\n내용\\n휴대전화 번호\\n신청\\n1사용목적\\n2사용대수\\n3사용기간\\n1일 사용 신청서\\n신.\n",
    "\"\"\"\n",
    "\n",
    "result_3 = \"\"\"\n",
    "중앙행정과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '성별영향평가'를 실시하기 위해 성별, 연령, 성별 등 성별을 고려한 계획을 작성하여 \n",
    "각 기관에 제출할 수 있는 리스트를 작성해 '여성ᆞ성평등을 위한 종합계획' 및 '2020년까지 5개년 계획에 대한 성별영향분석리스트'를 작성하고 평가 대상자를 대상으로 ‘비전과 목표, 전략 및 중점과제’ 등을 작성해야 한다.  \n",
    "\"\"\"\n",
    "\n",
    "result_4 = \"\"\"\n",
    "중앙기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '성별영향평가'를 실시하기 위해 성별영향평가서를 작성하여 각 기관에 제출할 예정이며, 평가 시기 : 모든 과정은 GIA시스템을 통해 진행되고 \n",
    "\\n- 신규 수립인 경우 ‘계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후’ 중 빠른 시기에 비전과 목표, 전략 및 중점과제에 대한 「성별영향평가서」를 작성해 여성가족부 등 관계기관에 발송한다.\n",
    "\"\"\"\n",
    "\n",
    "result_5 = \"\"\"\n",
    "중앙행정과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '성별영향평가'를 실시하기 위해 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후' 중 빠른 시기에 비전과 목표, 전략 및 중점과제에 대한 「성별영향평가서」를 작성해 여성가족부 등 관계기관에 발송할 예정이다.\n",
    "\"\"\"\n",
    "\n",
    "result_6 = \"\"\"\n",
    "중앙행정기관과 지방자치단체는 법률에 따라 3년 이상의 주기로 수립하는 계획인 '성별영향평가'를 실시하기 위해 '계획 확정 3개월 전, 위원회 상정 60일 이전 또는 중간보고 후' 중 빠른 시기에 비전과 목표, 전략 및 중점과제에 대한 「성별영향평가서」를 작성해 여성ᆞ가족부 등에 발송할 예정이다.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
